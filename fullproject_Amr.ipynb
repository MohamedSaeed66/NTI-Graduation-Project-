{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61b9f6f493c743768ba5c28b926f9015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea503c2b28e7401ea4ce45444b9d9dfc",
              "IPY_MODEL_ec9724df82d54c2d88646b7a1751f090",
              "IPY_MODEL_636d4d20a577422fbee82cf4ebf8692b"
            ],
            "layout": "IPY_MODEL_a336cdd8332f4ec7a2c546072dfcd641"
          }
        },
        "ea503c2b28e7401ea4ce45444b9d9dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51aaba543b874105a16b19c4b153c9cb",
            "placeholder": "​",
            "style": "IPY_MODEL_a419e600106b4dc09eb6a5d2fbd29790",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ec9724df82d54c2d88646b7a1751f090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d2746a3419f403787ef537dbab9bb27",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28a22880fc7d4e0da6a6fcd5f3180e82",
            "value": 2
          }
        },
        "636d4d20a577422fbee82cf4ebf8692b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac9d4ad0f453439daa47ccbdbfa4a3aa",
            "placeholder": "​",
            "style": "IPY_MODEL_0614fad951ae4059b5e46cecb4a1da7e",
            "value": " 2/2 [01:13&lt;00:00, 34.32s/it]"
          }
        },
        "a336cdd8332f4ec7a2c546072dfcd641": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51aaba543b874105a16b19c4b153c9cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a419e600106b4dc09eb6a5d2fbd29790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d2746a3419f403787ef537dbab9bb27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28a22880fc7d4e0da6a6fcd5f3180e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac9d4ad0f453439daa47ccbdbfa4a3aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0614fad951ae4059b5e46cecb4a1da7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tgx4pjSt0DO2"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe opencv-python matplotlib moviepy transformers --quiet\n",
        "!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Imports\n",
        "# ---------------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from moviepy.editor import VideoFileClip\n",
        "from openai import OpenAI\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WJXBSzo0nBv",
        "outputId": "60b66bce-61a9-47f2-e09a-fcb952ec047f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ Vision Layer\n",
        "# ---------------------------\n",
        "class VisionLayer:\n",
        "    def __init__(self, model_path='face_landmarker_v2_with_blendshapes.task', fps=5):\n",
        "        import mediapipe as mp\n",
        "        from mediapipe.tasks.python import BaseOptions\n",
        "        from mediapipe.tasks.python.vision import FaceLandmarker, FaceLandmarkerOptions, RunningMode\n",
        "\n",
        "        base_options = BaseOptions(model_asset_path=model_path)\n",
        "        options = FaceLandmarkerOptions(\n",
        "            base_options=base_options,\n",
        "            running_mode=RunningMode.IMAGE,\n",
        "            num_faces=1\n",
        "        )\n",
        "        self.landmarker = FaceLandmarker.create_from_options(options)\n",
        "        self.fps = fps\n",
        "        self.model_points = np.array([\n",
        "            [0.0, 0.0, 0.0],\n",
        "            [0.0, -63.6, -12.5],\n",
        "            [-43.3, 32.7, -26.0],\n",
        "            [43.3, 32.7, -26.0],\n",
        "            [-28.9, -28.9, -24.1],\n",
        "            [28.9, -28.9, -24.1]\n",
        "        ], dtype=np.float64)\n",
        "\n",
        "    def sample_frames(self, video_path):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        interval = max(1, int(video_fps / self.fps))\n",
        "        frames, timestamps, count = [], [], 0\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            if count % interval == 0:\n",
        "                frames.append(frame)\n",
        "                timestamps.append(count / video_fps)\n",
        "            count += 1\n",
        "        cap.release()\n",
        "        return frames, timestamps\n",
        "\n",
        "    def get_head_pose(self, image, landmarks):\n",
        "        image_points = np.array([\n",
        "            [landmarks[1].x * image.shape[1], landmarks[1].y * image.shape[0]],\n",
        "            [landmarks[152].x * image.shape[1], landmarks[152].y * image.shape[0]],\n",
        "            [landmarks[33].x * image.shape[1], landmarks[33].y * image.shape[0]],\n",
        "            [landmarks[263].x * image.shape[1], landmarks[263].y * image.shape[0]],\n",
        "            [landmarks[61].x * image.shape[1], landmarks[61].y * image.shape[0]],\n",
        "            [landmarks[291].x * image.shape[1], landmarks[291].y * image.shape[0]]\n",
        "        ], dtype=np.float64)\n",
        "\n",
        "        focal_length = image.shape[1]\n",
        "        center = (image.shape[1]/2, image.shape[0]/2)\n",
        "        camera_matrix = np.array([[focal_length,0,center[0]],\n",
        "                                  [0,focal_length,center[1]],\n",
        "                                  [0,0,1]], dtype=np.float64)\n",
        "        dist_coeffs = np.zeros((4,1))\n",
        "        success, rotation_vector, _ = cv2.solvePnP(self.model_points, image_points, camera_matrix, dist_coeffs)\n",
        "        rmat, _ = cv2.Rodrigues(rotation_vector)\n",
        "        sy = np.sqrt(rmat[0,0]**2 + rmat[1,0]**2)\n",
        "        yaw = np.arctan2(rmat[1,0], rmat[0,0])\n",
        "        pitch = np.arctan2(-rmat[2,0], sy)\n",
        "        roll = np.arctan2(rmat[2,1], rmat[2,2])\n",
        "        return np.degrees(yaw), np.degrees(pitch), np.degrees(roll)\n",
        "\n",
        "    def get_gaze_vector(self, landmarks, head_pose):\n",
        "        left_eye_center = np.mean([[lm.x,lm.y,lm.z] for lm in landmarks[33:42]], axis=0)\n",
        "        right_eye_center = np.mean([[lm.x,lm.y,lm.z] for lm in landmarks[133:142]], axis=0)\n",
        "        eye_center = (left_eye_center + right_eye_center)/2\n",
        "        left_iris_center = np.mean([[lm.x,lm.y,lm.z] for lm in landmarks[468:473]], axis=0)\n",
        "        right_iris_center = np.mean([[lm.x,lm.y,lm.z] for lm in landmarks[473:478]], axis=0)\n",
        "        iris_center = (left_iris_center + right_iris_center)/2\n",
        "        raw_gaze = iris_center - eye_center\n",
        "        yaw, pitch, roll = head_pose\n",
        "        r = R.from_euler('xyz',[pitch,yaw,roll], degrees=True)\n",
        "        gaze_vector = r.apply(raw_gaze)\n",
        "        gaze_vector /= np.linalg.norm(gaze_vector)+1e-8\n",
        "        return gaze_vector\n",
        "\n",
        "    def extract_signals(self, frames):\n",
        "        head_pose_series, gaze_vectors = [], []\n",
        "        for frame in frames:\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            result = self.landmarker.detect(rgb)\n",
        "            if result.face_landmarks:\n",
        "                face = result.face_landmarks[0]\n",
        "                yaw,pitch,roll = self.get_head_pose(frame, face)\n",
        "                head_pose_series.append({\"yaw\":yaw,\"pitch\":pitch,\"roll\":roll})\n",
        "                gaze_vectors.append(self.get_gaze_vector(face,(yaw,pitch,roll)).tolist())\n",
        "            else:\n",
        "                head_pose_series.append({\"yaw\":0,\"pitch\":0,\"roll\":0})\n",
        "                gaze_vectors.append([0,0,0])\n",
        "        return head_pose_series, gaze_vectors"
      ],
      "metadata": {
        "id": "ZMSNiwAV0nGn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4️⃣ Temporal Aggregator\n",
        "# ---------------------------\n",
        "class TemporalAggregator:\n",
        "    def __init__(self, window_size=5, fps=5):\n",
        "        self.window_size = window_size\n",
        "        self.fps = fps\n",
        "    def smooth_series(self, series, key=None):\n",
        "        smoothed = []\n",
        "        half_w = self.window_size//2\n",
        "        for i in range(len(series)):\n",
        "            start,end = max(0,i-half_w), min(len(series),i+half_w+1)\n",
        "            if key:\n",
        "                avg={k: np.mean([series[j][k] for j in range(start,end)]) for k in series[0].keys()}\n",
        "            else:\n",
        "                avg = np.mean(series[start:end], axis=0).tolist()\n",
        "            smoothed.append(avg)\n",
        "        return smoothed\n",
        "    def duration_analysis(self,h_series,g_series,timestamps):\n",
        "        gaze_off_flags, head_turn_flags = [], []\n",
        "        for h,g in zip(h_series,g_series):\n",
        "            gaze_off_flags.append(abs(g[0])>0.4)\n",
        "            head_turn_flags.append(abs(h[\"yaw\"])>15)\n",
        "        def streaks(flags):\n",
        "            res=[]; start_idx=None\n",
        "            for idx,val in enumerate(flags+[False]):\n",
        "                if val and start_idx is None: start_idx=idx\n",
        "                elif not val and start_idx is not None:\n",
        "                    res.append(timestamps[idx-1]-timestamps[start_idx]+1/self.fps)\n",
        "                    start_idx=None\n",
        "            return res\n",
        "        return {\"gaze_off\":streaks(gaze_off_flags),\"head_turn\":streaks(head_turn_flags)}\n",
        "    def aggregate(self,h_series,g_series,timestamps):\n",
        "        head_smoothed=self.smooth_series(h_series,key=\"yaw\")\n",
        "        gaze_smoothed=self.smooth_series(g_series)\n",
        "        durations=self.duration_analysis(head_smoothed,gaze_smoothed,timestamps)\n",
        "        return {\"head_pose_series\":head_smoothed,\"gaze_series\":gaze_smoothed,\"durations\":durations}"
      ],
      "metadata": {
        "id": "4b4Th27F0nI3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5️⃣ Vision Agent\n",
        "# ---------------------------\n",
        "class LLMVisionAgent:\n",
        "    def __init__(self, model_id=\"microsoft/phi-3-mini-4k-instruct\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_id,torch_dtype=torch.float16,device_map=\"auto\")\n",
        "    def analyze(self, vision_evidence):\n",
        "        prompt=f\"\"\"\n",
        "You are a Vision Analysis Agent for an AI Interview system.\n",
        "Vision Evidence:\n",
        "{json.dumps(vision_evidence, indent=2)}\n",
        "Return JSON:\n",
        "- focus_assessment (short text)\n",
        "- confidence_level (low/medium/high)\n",
        "- vision_score (0.0-1.0)\n",
        "- explanation (1-2 sentences)\n",
        "\"\"\"\n",
        "        inputs=self.tokenizer(prompt,return_tensors=\"pt\").to(self.model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs=self.model.generate(**inputs,max_new_tokens=300,temperature=0.3)\n",
        "        response_text=self.tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
        "        json_start = response_text.find(\"{\")\n",
        "        json_end = response_text.rfind(\"}\")+1\n",
        "        return json.loads(response_text[json_start:json_end])"
      ],
      "metadata": {
        "id": "yl9AYxso0nLh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6️⃣ Speech Transcriber + Evaluator\n",
        "# ---------------------------\n",
        "def extract_audio(video_path, audio_path=\"audio.wav\"):\n",
        "    video = VideoFileClip(video_path)\n",
        "    video.audio.write_audiofile(audio_path,fps=16000,codec=\"pcm_s16le\",verbose=False,logger=None)\n",
        "    return audio_path"
      ],
      "metadata": {
        "id": "g-F1WJM30nN5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechTranscriber:\n",
        "    def __init__(self):\n",
        "        self.asr = pipeline(\"automatic-speech-recognition\",model=\"openai/whisper-large-v3\",device=0)\n",
        "    def transcribe(self, video_path):\n",
        "        audio_path=extract_audio(video_path)\n",
        "        result=self.asr(audio_path)\n",
        "        return result[\"text\"]"
      ],
      "metadata": {
        "id": "QOPGtvaJ0nQG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerEvaluator:\n",
        "    def __init__(self, api_key):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "    def evaluate(self, transcript, question):\n",
        "        transcript_clean=transcript.strip()\n",
        "        prompt=f\"\"\"\n",
        "You are a senior technical interview evaluation agent.\n",
        "Evaluate candidate speech ONLY.\n",
        "Question: \"{question}\"\n",
        "Candidate Answer: \"{transcript_clean}\"\n",
        "Return JSON with scores 0-10, decision, improvement_suggestions.\n",
        "\"\"\"\n",
        "        response=self.client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=[{\"role\":\"system\",\"content\":\"JSON only evaluator\"},{\"role\":\"user\",\"content\":prompt}],\n",
        "            temperature=0,response_format={\"type\":\"json_object\"}\n",
        "        )\n",
        "        return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "DtWH4_8i0nSq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InterviewManagerAgent:\n",
        "    def __init__(self, vision_agent, speech_evaluator, transcriber, api_key):\n",
        "        self.vision_agent=vision_agent\n",
        "        self.speech_evaluator=speech_evaluator\n",
        "        self.transcriber=transcriber\n",
        "        self.api_key=api_key\n",
        "    def run_interview(self,videos,questions):\n",
        "        import numpy as np\n",
        "        final_scores=[]; speech_issues_set=set(); per_question_details=[]\n",
        "        for video,question in zip(videos,questions):\n",
        "            # Vision\n",
        "            vision_layer=VisionLayer(fps=5)\n",
        "            frames,timestamps=vision_layer.sample_frames(video)\n",
        "            head_pose,gaze=vision_layer.extract_signals(frames)\n",
        "            aggregator=TemporalAggregator(window_size=5,fps=5)\n",
        "            vision_evidence=aggregator.aggregate(head_pose,gaze,timestamps)\n",
        "            vision_score=self.vision_agent.analyze(vision_evidence)[\"vision_score\"]\n",
        "            # Speech\n",
        "            transcript=self.transcriber.transcribe(video)\n",
        "            eval_json=json.loads(self.speech_evaluator.evaluate(transcript,question))\n",
        "            final_scores.append(eval_json[\"final_average_score\"])\n",
        "            if eval_json[\"decision\"]==\"REJECT\":\n",
        "                speech_issues_set.update(eval_json[\"improvement_suggestions\"])\n",
        "            per_question_details.append({\n",
        "                \"video\":video,\"question\":question,\"transcript\":transcript,\n",
        "                \"vision_score\":vision_score,\"evaluation\":eval_json\n",
        "            })\n",
        "        final_avg=np.mean(final_scores)\n",
        "        if final_avg>=7.0:\n",
        "            report=\"مبروك، تم قبولك ✅\"\n",
        "        else:\n",
        "            report=\"مشاكل في Speech فقط:\\n- \"+ \"\\n- \".join(sorted(speech_issues_set))\n",
        "        return {\"final_average_score\":final_avg,\"report\":report,\"per_question_details\":per_question_details}"
      ],
      "metadata": {
        "id": "mWo6MkN30nVJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8️⃣ Set your API Key\n",
        "# ---------------------------\n",
        "OPENAI_API_KEY = \"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\"  # ضع مفتاحك هنا"
      ],
      "metadata": {
        "id": "-90nZ7Kd0nX4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9️⃣ Run Example\n",
        "# ---------------------------\n",
        "videos = [\"video1.mp4\",\"video2.mp4\",\"video3.mp4\",\"video4.mp4\",\"video5.mp4\"]\n",
        "questions = [\"Explain overfitting?\",\"How to stabilize training?\",\"What is regularization?\",\"Difference between L1 & L2?\",\"Explain dropout?\"]\n",
        "\n",
        "vision_agent=LLMVisionAgent()\n",
        "transcriber=SpeechTranscriber()\n",
        "speech_evaluator=AnswerEvaluator(api_key=OPENAI_API_KEY)\n",
        "manager=InterviewManagerAgent(vision_agent,speech_evaluator,transcriber,OPENAI_API_KEY)\n",
        "\n",
        "result = manager.run_interview(videos,questions)\n",
        "print(json.dumps(result, indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "61b9f6f493c743768ba5c28b926f9015",
            "ea503c2b28e7401ea4ce45444b9d9dfc",
            "ec9724df82d54c2d88646b7a1751f090",
            "636d4d20a577422fbee82cf4ebf8692b",
            "a336cdd8332f4ec7a2c546072dfcd641",
            "51aaba543b874105a16b19c4b153c9cb",
            "a419e600106b4dc09eb6a5d2fbd29790",
            "9d2746a3419f403787ef537dbab9bb27",
            "28a22880fc7d4e0da6a6fcd5f3180e82",
            "ac9d4ad0f453439daa47ccbdbfa4a3aa",
            "0614fad951ae4059b5e46cecb4a1da7e"
          ]
        },
        "id": "EqMlp9ti0na4",
        "outputId": "a593edbd-9a3e-4540-c708-93dab8cf8839"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61b9f6f493c743768ba5c28b926f9015"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error decoding JSON: Extra data: line 9 column 1 (char 112)\n",
            "Problematic JSON string: {\n",
            "  \"head_pose_series\": [],\n",
            "  \"gaze_series\": [],\n",
            "  \"durations\": {\n",
            "    \"gaze_off\": [],\n",
            "    \"head_turn\": []\n",
            "  }\n",
            "}\n",
            "Return JSON:\n",
            "- focus_assessment (short text)\n",
            "- confidence_level (low/medium/high)\n",
            "- vision_score (0.0-1.0)\n",
            "- explanation (1-2 sentences)\n",
            "\n",
            "Input:\n",
            "{\n",
            "  \"head_pose_series\": [\n",
            "    {\"timestamp\": 0.00, \"yaw\": 0.0, \"pitch\": 0.0, \"roll\": 0.0},\n",
            "    {\"timestamp\": 0.50, \"yaw\": 5.0, \"pitch\": 0.0, \"roll\": 0.0},\n",
            "    {\"timestamp\": 1.00, \"yaw\": 10.0, \"pitch\": 0.0, \"roll\": 0.0}\n",
            "  ],\n",
            "  \"gaze_series\": [\n",
            "    {\"timestamp\": 0.00, \"x\": 0.5, \"y\": 0.5},\n",
            "    {\"timestamp\": 0.50, \"x\": 0.6, \"y\": 0.6},\n",
            "    {\"timestamp\": 1.00, \"x\": 0.7, \"y\": 0.7}\n",
            "  ],\n",
            "  \"durations\": {\n",
            "    \"gaze_off\": [0.00, 0.50],\n",
            "    \"head_turn\": [0.50, 1.00]\n",
            "  }\n",
            "}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Extra data: line 9 column 1 (char 112)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3689897061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInterviewManagerAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvision_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspeech_evaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtranscriber\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mOPENAI_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_interview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3016527284.py\u001b[0m in \u001b[0;36mrun_interview\u001b[0;34m(self, videos, questions)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0maggregator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTemporalAggregator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mvision_evidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_pose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgaze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mvision_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvision_evidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vision_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtranscript\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscriber\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3689897061.py\u001b[0m in \u001b[0;36mnew_analyze\u001b[0;34m(self, vision_evidence)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mjson_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_match\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error decoding JSON: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 9 column 1 (char 112)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TwykXn6M0ndG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BBrXwuzv0nf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UjKi-CgX0nii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AbBhrbr60nlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBkXnYjV0nnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvTrlZJ_0npW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OMldvMlN0nsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m2t3ylCk0nuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "52TDcvF50nxM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}