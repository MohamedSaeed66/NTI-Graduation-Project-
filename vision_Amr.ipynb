{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KyGBrNDhLPRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c8def2-3bb3-4d8d-f2e5-1211ba6b8162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\n",
        "!pip install mediapipe opencv-python matplotlib --quiet\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importations"
      ],
      "metadata": {
        "id": "vKGBEzNbLmEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mediapipe.tasks.python import BaseOptions\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "from mediapipe.tasks.python.vision import (\n",
        "    FaceLandmarker,\n",
        "    FaceLandmarkerOptions,\n",
        "    RunningMode\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "nq_JiLyoLWeZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Layer"
      ],
      "metadata": {
        "id": "qJreIGiiLhM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionLayer:\n",
        "    def __init__(self, model_path='face_landmarker_v2_with_blendshapes.task', fps=5):\n",
        "        base_options = BaseOptions(model_asset_path=model_path)\n",
        "        options = FaceLandmarkerOptions(\n",
        "            base_options=base_options,\n",
        "            running_mode=RunningMode.IMAGE,\n",
        "            num_faces=1\n",
        "        )\n",
        "        self.landmarker = FaceLandmarker.create_from_options(options)\n",
        "        self.fps = fps\n",
        "\n",
        "        # 3D head model points for solvePnP\n",
        "        self.model_points = np.array([\n",
        "            [0.0, 0.0, 0.0],          # Nose tip\n",
        "            [0.0, -63.6, -12.5],      # Chin\n",
        "            [-43.3, 32.7, -26.0],     # Left eye corner\n",
        "            [43.3, 32.7, -26.0],      # Right eye corner\n",
        "            [-28.9, -28.9, -24.1],    # Left mouth corner\n",
        "            [28.9, -28.9, -24.1]      # Right mouth corner\n",
        "        ], dtype=np.float64)\n",
        "            # ---------------------------\n",
        "    # Frame Sampler\n",
        "    # ---------------------------\n",
        "    def sample_frames(self, video_path):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        interval = max(1, int(video_fps / self.fps))\n",
        "        frames = []\n",
        "        timestamps = []\n",
        "        count = 0\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if count % interval == 0:\n",
        "                frames.append(frame)\n",
        "                timestamps.append(count / video_fps)\n",
        "            count += 1\n",
        "        cap.release()\n",
        "        return frames, timestamps\n",
        "\n",
        "    # ---------------------------\n",
        "    # Head Pose (yaw, pitch, roll)\n",
        "    # ---------------------------\n",
        "    def get_head_pose(self, image, landmarks):\n",
        "        image_points = np.array([\n",
        "            [landmarks[1].x * image.shape[1], landmarks[1].y * image.shape[0]],   # Nose tip\n",
        "            [landmarks[152].x * image.shape[1], landmarks[152].y * image.shape[0]], # Chin\n",
        "            [landmarks[33].x * image.shape[1], landmarks[33].y * image.shape[0]],   # Left eye\n",
        "            [landmarks[263].x * image.shape[1], landmarks[263].y * image.shape[0]], # Right eye\n",
        "            [landmarks[61].x * image.shape[1], landmarks[61].y * image.shape[0]],   # Left mouth\n",
        "            [landmarks[291].x * image.shape[1], landmarks[291].y * image.shape[0]]  # Right mouth\n",
        "        ], dtype=np.float64)\n",
        "\n",
        "        focal_length = image.shape[1]\n",
        "        center = (image.shape[1]/2, image.shape[0]/2)\n",
        "        camera_matrix = np.array([[focal_length,0,center[0]],\n",
        "                                  [0,focal_length,center[1]],\n",
        "                                  [0,0,1]], dtype=np.float64)\n",
        "        dist_coeffs = np.zeros((4,1))\n",
        "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
        "            self.model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE\n",
        "        )\n",
        "        rmat, _ = cv2.Rodrigues(rotation_vector)\n",
        "        sy = np.sqrt(rmat[0,0]**2 + rmat[1,0]**2)\n",
        "        yaw = np.arctan2(rmat[1,0], rmat[0,0])\n",
        "        pitch = np.arctan2(-rmat[2,0], sy)\n",
        "        roll = np.arctan2(rmat[2,1], rmat[2,2])\n",
        "        return np.degrees(yaw), np.degrees(pitch), np.degrees(roll)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Gaze Direction (3D Approx)\n",
        "    # ---------------------------\n",
        "    # def get_gaze_vector(self, landmarks, head_pose):\n",
        "    #     # iris x avg\n",
        "    #     left_iris_x = np.mean([lm.x for lm in landmarks[468:473]])\n",
        "    #     right_iris_x = np.mean([lm.x for lm in landmarks[473:478]])\n",
        "    #     avg_x = (left_iris_x + right_iris_x)/2\n",
        "    #     # gaze 3D approximation\n",
        "    #     yaw, pitch, _ = head_pose\n",
        "    #     gaze_vector = [avg_x, pitch/90, yaw/90]  # normalized\n",
        "    #     return gaze_vector\n",
        "\n",
        "    def get_gaze_vector(self, landmarks, head_pose):\n",
        "        \"\"\"\n",
        "        Compute a more accurate gaze vector using iris landmarks and head pose.\n",
        "\n",
        "        landmarks: list of mediapipe landmarks\n",
        "        head_pose: tuple (yaw, pitch, roll) in degrees\n",
        "        \"\"\"\n",
        "        # 1′′ Eye centers approximation\n",
        "        left_eye_center = np.mean([[lm.x, lm.y, lm.z] for lm in landmarks[33:42]], axis=0)\n",
        "        right_eye_center = np.mean([[lm.x, lm.y, lm.z] for lm in landmarks[133:142]], axis=0)\n",
        "        eye_center = (left_eye_center + right_eye_center) / 2\n",
        "\n",
        "        # 2′′ Iris center\n",
        "        left_iris_center = np.mean([[lm.x, lm.y, lm.z] for lm in landmarks[468:473]], axis=0)\n",
        "        right_iris_center = np.mean([[lm.x, lm.y, lm.z] for lm in landmarks[473:478]], axis=0)\n",
        "        iris_center = (left_iris_center + right_iris_center) / 2\n",
        "\n",
        "        # 3′′ Raw gaze vector (from eye center to iris center)\n",
        "        raw_gaze = iris_center - eye_center\n",
        "\n",
        "        # 4′′ Rotate gaze according to head pose (yaw, pitch, roll)\n",
        "        yaw, pitch, roll = head_pose  # degrees\n",
        "        r = R.from_euler('xyz', [pitch, yaw, roll], degrees=True)\n",
        "        gaze_vector = r.apply(raw_gaze)\n",
        "\n",
        "        # 5′′ Normalize\n",
        "        gaze_vector /= np.linalg.norm(gaze_vector) + 1e-8\n",
        "\n",
        "        return gaze_vector\n",
        "\n",
        "\n",
        "    # ---------------------------\n",
        "    # Extract Signals per Frame\n",
        "    # ---------------------------\n",
        "    def extract_signals(self, frames):\n",
        "        head_pose_series = []\n",
        "        gaze_vectors = []\n",
        "        timestamps = []\n",
        "        for idx, frame in enumerate(frames):\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            result = self.landmarker.detect(mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb))\n",
        "            if result.face_landmarks:\n",
        "                face = result.face_landmarks[0]\n",
        "                head_pose = {}\n",
        "                yaw, pitch, roll = self.get_head_pose(frame, face)\n",
        "                head_pose = {\"yaw\": yaw, \"pitch\": pitch, \"roll\": roll}\n",
        "                gaze = self.get_gaze_vector(face, (yaw,pitch,roll))\n",
        "\n",
        "                head_pose_series.append(head_pose)\n",
        "                gaze_vectors.append(gaze)\n",
        "            else:\n",
        "                head_pose_series.append({\"yaw\":0,\"pitch\":0,\"roll\":0})\n",
        "                gaze_vectors.append([0,0,0])\n",
        "        return head_pose_series, gaze_vectors"
      ],
      "metadata": {
        "id": "1rwVgLmoLWbg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalAggregator:\n",
        "    def __init__(self, window_size=5, fps=5):\n",
        "        self.window_size = window_size  # for smoothing\n",
        "        self.fps = fps\n",
        "\n",
        "    # ---------------------------\n",
        "    # Simple moving average smoothing\n",
        "    # ---------------------------\n",
        "    def smooth_series(self, series, key=None):\n",
        "        smoothed = []\n",
        "        half_w = self.window_size // 2\n",
        "        for i in range(len(series)):\n",
        "            start = max(0, i - half_w)\n",
        "            end = min(len(series), i + half_w + 1)\n",
        "            if key:  # dict series (head_pose)\n",
        "                avg = {k: np.mean([series[j][k] for j in range(start, end)]) for k in series[0].keys()}\n",
        "            else:   # list/array series (gaze vector)\n",
        "                avg = np.mean(series[start:end], axis=0).tolist()\n",
        "            smoothed.append(avg)\n",
        "        return smoothed\n",
        "\n",
        "    # ---------------------------\n",
        "    # Duration analysis\n",
        "    # ---------------------------\n",
        "    def duration_analysis(self, head_pose_series, gaze_series, timestamps):\n",
        "        gaze_off_flags = []\n",
        "        head_turn_flags = []\n",
        "\n",
        "        for h, g in zip(head_pose_series, gaze_series):\n",
        "            yaw = h[\"yaw\"]\n",
        "            pitch = h[\"pitch\"]\n",
        "            gx = g[0]\n",
        "\n",
        "            # gaze off camera if |x| > 0.4\n",
        "            gaze_off_flags.append(abs(gx) > 0.4)\n",
        "            # head turned if |yaw| > 15°\n",
        "            head_turn_flags.append(abs(yaw) > 15)\n",
        "\n",
        "        def streaks(flags):\n",
        "            streaks_list = []\n",
        "            start_idx = None\n",
        "            for idx, val in enumerate(flags + [False]):  # add False at end to flush last streak\n",
        "                if val and start_idx is None:\n",
        "                    start_idx = idx\n",
        "                elif not val and start_idx is not None:\n",
        "                    duration = timestamps[idx-1] - timestamps[start_idx] + 1/self.fps\n",
        "                    streaks_list.append(duration)\n",
        "                    start_idx = None\n",
        "            return streaks_list\n",
        "\n",
        "        gaze_off_durations = streaks(gaze_off_flags)\n",
        "        head_turn_durations = streaks(head_turn_flags)\n",
        "\n",
        "        return {\"gaze_off\": gaze_off_durations, \"head_turn\": head_turn_durations}\n",
        "\n",
        "    # Full Aggregation\n",
        "    # ---------------------------\n",
        "    def aggregate(self, head_pose_series, gaze_series, timestamps):\n",
        "        head_smoothed = self.smooth_series(head_pose_series, key=\"yaw\")\n",
        "        gaze_smoothed = self.smooth_series(gaze_series)\n",
        "\n",
        "        durations = self.duration_analysis(head_smoothed, gaze_smoothed, timestamps)\n",
        "        # Vision Evidence\n",
        "        evidence = {\n",
        "            \"head_pose_series\": head_smoothed,\n",
        "            \"gaze_series\": gaze_smoothed,\n",
        "            \"durations\": durations,\n",
        "        }\n",
        "        return evidence"
      ],
      "metadata": {
        "id": "CJXy454Qbf3g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Video path"
      ],
      "metadata": {
        "id": "v7CXBEYMX_Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/WIN_20251230_23_07_56_Pro.mp4\"\n",
        "\n",
        "# 1️⃣ Vision Layer\n",
        "vision = VisionLayer(fps=5)\n",
        "frames, timestamps = vision.sample_frames(video_path)\n",
        "head_pose_series, gaze_series = vision.extract_signals(frames)\n",
        "\n",
        "# 2️⃣ Temporal Aggregator\n",
        "aggregator = TemporalAggregator(window_size=5, fps=5)\n",
        "vision_evidence = aggregator.aggregate(head_pose_series, gaze_series, timestamps)\n",
        "\n",
        "# 3️⃣ Output\n",
        "print(\"=== Vision Evidence ===\")\n",
        "print(\"Head Pose (first 5 frames):\", head_pose_series[:5])\n",
        "print(\"Gaze Vectors (first 5 frames):\", gaze_series[:5])\n",
        "print(\"Durations:\", vision_evidence[\"durations\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRDR7BeUPePn",
        "outputId": "7f451969-a3b7-4036-ce6c-b853a51039d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Vision Evidence ===\n",
            "Head Pose (first 5 frames): [{'yaw': np.float64(-1.3155129896432056), 'pitch': np.float64(18.178739447173847), 'roll': np.float64(-173.39582807747377)}, {'yaw': np.float64(-2.197778235398298), 'pitch': np.float64(16.33270941386751), 'roll': np.float64(-173.189405732221)}, {'yaw': np.float64(-1.3865676248183163), 'pitch': np.float64(22.63606521260831), 'roll': np.float64(-173.97837422808524)}, {'yaw': np.float64(-1.428939702351513), 'pitch': np.float64(14.244271738967104), 'roll': np.float64(-178.74785877919754)}, {'yaw': np.float64(-1.5184295958794545), 'pitch': np.float64(-26.02123126524632), 'roll': np.float64(-175.97438399057637)}]\n",
            "Gaze Vectors (first 5 frames): [array([-0.45080724,  0.81019013, -0.37465265]), array([-0.45212392,  0.8262496 , -0.33599908]), array([-0.42864197,  0.78795826, -0.44202677]), array([-0.36904176,  0.87735788, -0.30667761]), array([-0.38993666,  0.84786006,  0.35928061])]\n",
            "Durations: {'gaze_off': [0.798519024390244, 2.3945697560975616, 0.9980253658536584, 0.3995063414634144, 0.9980253658536593, 1.1975317073170728, 0.7985190243902422, 0.3995063414634153], 'head_turn': [1.5965443902439025, 0.9980253658536584, 2.5940760975609765, 0.2, 0.3995063414634153, 1.995557073170734, 0.5990126829268305]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EvaluationAgent"
      ],
      "metadata": {
        "id": "j8HF6KOWYNCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "class LLMVisionAgent:\n",
        "    def __init__(self, model_id=\"microsoft/phi-3-mini-4k-instruct\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    def analyze(self, vision_evidence):\n",
        "        prompt = f\"\"\"\n",
        "You are a Vision Analysis Agent for an AI Interview system.\n",
        "\n",
        "You receive structured visual evidence extracted from a candidate video.\n",
        "Your job is NOT to accuse cheating.\n",
        "Your job is to assess focus, attentiveness, and confidence.\n",
        "\n",
        "Vision Evidence:\n",
        "{json.dumps(vision_evidence, indent=2)}\n",
        "\n",
        "Return a JSON object with:\n",
        "- focus_assessment (short text)\n",
        "- confidence_level (low / medium / high)\n",
        "- vision_score (0.0 to 1.0)\n",
        "- explanation (1-2 sentences)\n",
        "\n",
        "Be conservative. Avoid strong claims.\n",
        "\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=300,\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "        response_text = self.tokenizer.decode(\n",
        "            outputs[0],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        # مهم: اساءراء الـ JSON من الراء\n",
        "        # The model often echoes the prompt. We need to extract only the generated JSON.\n",
        "        decoded_prompt = self.tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "\n",
        "        generated_content = \"\"\n",
        "        # Check if the response_text starts with the decoded prompt\n",
        "        if response_text.startswith(decoded_prompt):\n",
        "            generated_content = response_text[len(decoded_prompt):].strip()\n",
        "        else:\n",
        "            # Fallback: if not starting with prompt, assume the relevant JSON is the last one\n",
        "            # or try to find it within the whole response_text\n",
        "            generated_content = response_text.strip()\n",
        "\n",
        "        json_start = generated_content.find(\"{\")\n",
        "        json_end = generated_content.rfind(\"}\") + 1\n",
        "\n",
        "        if json_start != -1 and json_end != -1 and json_end > json_start:\n",
        "            try:\n",
        "                json_string = generated_content[json_start:json_end]\n",
        "                return json.loads(json_string)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSON Decode Error: {e}\")\n",
        "                print(f\"Attempted to decode: {json_string}\")\n",
        "                raise\n",
        "        else:\n",
        "            print(\"No valid JSON found in LLM's generated response.\")\n",
        "            print(f\"Generated Content: {generated_content}\")\n",
        "            # As a last resort, if no valid JSON is found, return an error or a default structure\n",
        "            raise ValueError(\"LLM did not return a parseable JSON analysis.\")"
      ],
      "metadata": {
        "id": "Arerya3jLWVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vision_agent = LLMVisionAgent(\n",
        "    model_id=\"microsoft/phi-3-mini-4k-instruct\"  # Corrected argument name\n",
        ")\n",
        "\n",
        "result = vision_agent.analyze(vision_evidence)\n",
        "print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "id": "5CzSwuxGLWTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xjt992yxLWRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vvwkTdFKLWPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSRZO-peLWN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}